{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "klFm9qcB8aRR",
        "RfzSpMGxMpJk",
        "I1_1lvc3MZrS",
        "h4icUqqgMi44",
        "EzjLOyU6M57W",
        "WJDybT9ANBOm",
        "skODRpAiNLjW",
        "cXyPKwBCoKwG",
        "ydoqdIAX37PA",
        "NSbOWU9u4W0q",
        "JsZV1eDkJaki",
        "fMaZTeUSK-TP"
      ],
      "authorship_tag": "ABX9TyNMHeEng7ueOzgNrSU6MMfT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alwaysneedhelp/AI-Challenge/blob/main/advertisement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Importing Necessary Libraries***"
      ],
      "metadata": {
        "id": "klFm9qcB8aRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNrxF_YSsLR0",
        "outputId": "8581372c-8115-43df-9154-7338c24fe310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import matplotlib as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import random\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Loading Training Data***"
      ],
      "metadata": {
        "id": "rcYLo5vb7vHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Extracting needed files from google drive***"
      ],
      "metadata": {
        "id": "RfzSpMGxMpJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the train.csv from mydrive to local colab, so it is easier to work with it\n",
        "\n",
        "if not (os.path.isfile('train.csv')):\n",
        "  !cp '/content/drive/MyDrive/AI_Challenge_advertisement_train.csv' train.csv\n",
        "\n",
        "df = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "QH6e8cSPsbou"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all the images from drive to my folder\n",
        "\n",
        "if not (os.path.exists('./images')):\n",
        "  !cp '/content/drive/MyDrive/images.zip' . # Using cp extract the file from google drive to a local folder\n",
        "  with zipfile.ZipFile('images.zip', 'r') as f:\n",
        "    f.extractall('./images')"
      ],
      "metadata": {
        "id": "os3ckJSC3RcB"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Making sure the Results are Reproducible***"
      ],
      "metadata": {
        "id": "I1_1lvc3MZrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the results reproducible using seed\n",
        "\n",
        "SEED = 42\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "metadata": {
        "id": "f7X3VAevuL5G"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Splitting Data and Preparing the Augmentetion***"
      ],
      "metadata": {
        "id": "h4icUqqgMi44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking some part of the data for validation, as we don't necessarily need it\n",
        "# (Since there is a lot of training data)\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state = SEED\n",
        ")"
      ],
      "metadata": {
        "id": "wn21_01pGdDS"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(5),\n",
        "    T.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
        "    T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "A7mJ1LXYrdzL"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Creating Dataset***"
      ],
      "metadata": {
        "id": "EzjLOyU6M57W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Img_Dataset(Dataset):\n",
        "  def __init__(self, df, transform=None, img_dir='./images', labels=True):\n",
        "    self.file_names = df['image'].tolist()\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.labels = False\n",
        "\n",
        "    # Just to not create a separate class for test\n",
        "    # Adding extra-check if labels exist in the given df\n",
        "\n",
        "    if labels:\n",
        "      self.labels = df['class'].tolist()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_names)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.file_names[idx])\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    if self.labels:\n",
        "      label = self.labels[idx]\n",
        "      return image, label, self.file_names[idx]\n",
        "\n",
        "    return image, self.file_names[idx]"
      ],
      "metadata": {
        "id": "ZMyJSnKk_qKL"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Img_Dataset(\n",
        "    train_df,\n",
        "    transform = train_transform)\n",
        "val_dataset = Img_Dataset(\n",
        "    val_df,\n",
        "    transform = val_transform)"
      ],
      "metadata": {
        "id": "D4HMjWD-J717"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Load Data from Dataset***"
      ],
      "metadata": {
        "id": "WJDybT9ANBOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a loder to load all the data\n",
        "\n",
        "# Trying different batches starting off with 8, since we have a lot of data, but still the dataset isn't that large\n",
        "\n",
        "BATCH = 8\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = BATCH,\n",
        "    shuffle = True, # Shuffling data when loading it\n",
        "    num_workers = 2, # Put 2 workers, so that process goes faster\n",
        "    drop_last = True # Drop everything that remains after placing data into batches of 8\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = BATCH,\n",
        "    shuffle = True,\n",
        "    num_workers = 2,\n",
        "    drop_last = True\n",
        ")"
      ],
      "metadata": {
        "id": "zfla8BRVTJ7m"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Train the Model***"
      ],
      "metadata": {
        "id": "skODRpAiNLjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Use Pretrained ResNet50 Model as BackBone***"
      ],
      "metadata": {
        "id": "cXyPKwBCoKwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model():\n",
        "\n",
        "    # Load a pretrained ResNet50 backbone\n",
        "    m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "    # Getting how many features were input for the lasat layer\n",
        "    in_features = m.fc.in_features\n",
        "\n",
        "    # Replace the 1000-class head with a single-logit head\n",
        "    m.fc = nn.Linear(in_features, 1)\n",
        "    return m\n",
        "\n",
        "\n",
        "model = prepare_model()"
      ],
      "metadata": {
        "id": "kuj31JqWoGmu"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Preparing Everything Needed for Training***"
      ],
      "metadata": {
        "id": "ydoqdIAX37PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha  # e.g., alpha = n_neg/(n_pos+n_neg) or 0.75\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # logits: [B,1], targets: [B,1] float in {0,1}\n",
        "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
        "        p = torch.sigmoid(logits)\n",
        "        pt = p*targets + (1-p)*(1-targets)     # pt = p if y=1 else 1-p\n",
        "        mod = (1 - pt).pow(self.gamma)         # focusing term\n",
        "        if self.alpha is not None:\n",
        "            alpha_t = self.alpha*targets + (1-self.alpha)*(1-targets)\n",
        "            bce = alpha_t * bce\n",
        "        return (mod * bce).mean()"
      ],
      "metadata": {
        "id": "5GrH6FkqD971"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Making balanced loss function, since there is class imbalance in the dataset\n",
        "n_pos = (train_df['class'] == 1).sum()\n",
        "n_neg = (train_df['class'] == 0).sum()\n",
        "\n",
        "\n",
        "# Try to use not just simple BCEWithLogitsLoss, but Focal Loss\n",
        "# Since there is severe imbalance in classes\n",
        "alpha = n_neg / (n_pos + n_neg)\n",
        "criterion = FocalLoss(gamma=2.0, alpha=alpha)\n",
        "\n",
        "\n",
        "\n",
        "# Use the common, simple Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Use scheduler to regulate lr\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "\n",
        "# Use scaler to reduce the memory usage\n",
        "scaler = torch.amp.GradScaler(DEVICE)"
      ],
      "metadata": {
        "id": "zJB7NMcyrNzY"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, scaler, epochs):\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    best_val_f1 = -1.0\n",
        "    best_threshold = 0.5\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch+1}')\n",
        "\n",
        "        # ---- TRAIN ----\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0.0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        for imgs, labels, _ in train_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE).view(-1, 1).float()\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                outputs = model(imgs)                        # [B,1]\n",
        "                loss = criterion(outputs, labels)            # BCEWithLogitsLoss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            if DEVICE == \"cuda\":\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Train metrics @ fixed 0.5 (just for logging)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).int()\n",
        "            correct += (preds == labels.int()).sum().item()\n",
        "            all_preds.extend(preds.view(-1).cpu().numpy().tolist())\n",
        "            all_labels.extend(labels.view(-1).cpu().numpy().astype(int).tolist())\n",
        "\n",
        "        running_loss /= len(train_loader)\n",
        "        train_acc = 100.0 * correct / len(train_loader.dataset)\n",
        "        train_f1 = f1_score(all_labels, all_preds, average='binary')\n",
        "\n",
        "        print(f'Training Loss: {running_loss:.4f}')\n",
        "        print(f'Training Accuracy: {train_acc:.2f}%')\n",
        "        print(f'Training F1 @0.5: {train_f1:.4f}')\n",
        "\n",
        "        # Validating at the same time while training\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_probs = []\n",
        "            val_labels = []\n",
        "            for imgs, labels, _ in val_loader:\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                logits = model(imgs)\n",
        "                probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()\n",
        "                val_probs.extend(probs.tolist())\n",
        "                val_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "        val_probs = np.array(val_probs)\n",
        "        val_labels = np.array(val_labels)\n",
        "\n",
        "        # Try different thresholds to maximize F1 score\n",
        "        best_t_epoch = 0.5\n",
        "        best_f1_epoch = 0.0\n",
        "        for t in np.linspace(0.05, 0.95, 19):\n",
        "            preds = (val_probs >= t).astype(int)\n",
        "            f1 = f1_score(val_labels, preds, average='binary')\n",
        "            if f1 > best_f1_epoch:\n",
        "                best_f1_epoch = f1\n",
        "                best_t_epoch = t\n",
        "\n",
        "        print(f'Validation F1 (best): {best_f1_epoch:.4f} @ threshold {best_t_epoch:.2f}')\n",
        "\n",
        "        # keep the best across epochs\n",
        "        if best_f1_epoch > best_val_f1:\n",
        "            best_val_f1 = best_f1_epoch\n",
        "            best_threshold = best_t_epoch\n",
        "\n",
        "    print(f'Best Val F1 over training: {best_val_f1:.4f} @ threshold {best_threshold:.2f}')\n",
        "    return model, best_threshold, best_val_f1\n"
      ],
      "metadata": {
        "id": "W1cF2JGAvgjx"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Training***"
      ],
      "metadata": {
        "id": "NSbOWU9u4W0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, best_t, best_val_f1 = train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    scaler,\n",
        "    3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNLN1m_J3Uqd",
        "outputId": "7bfde1f7-8f04-44b7-8bb9-8d9562b378a0"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1595521608.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0161\n",
            "Training Accuracy: 91.11%\n",
            "Training F1 @0.5: 0.9591\n",
            "Validation F1 (best): 1.0000 @ threshold 0.25\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1595521608.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0029\n",
            "Training Accuracy: 96.67%\n",
            "Training F1 @0.5: 0.9933\n",
            "Validation F1 (best): 1.0000 @ threshold 0.15\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1595521608.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0019\n",
            "Training Accuracy: 97.04%\n",
            "Training F1 @0.5: 0.9956\n",
            "Validation F1 (best): 1.0000 @ threshold 0.30\n",
            "Best Val F1 over training: 1.0000 @ threshold 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solid results on training"
      ],
      "metadata": {
        "id": "r6UJyBnbJYIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Save This Model***"
      ],
      "metadata": {
        "id": "fMaZTeUSK-TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not (os.path.isfile('model.pth')):\n",
        "  torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "wnHYqTAELCTH"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Now, Predict the Results for Test Data***"
      ],
      "metadata": {
        "id": "4T0oLWKeL1y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Extract Files from GoogleDrive first***"
      ],
      "metadata": {
        "id": "HAMgQpFWMIor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the test.csv from mydrive to local colab, so it is easier to work with it\n",
        "\n",
        "if not (os.path.isfile('test.csv')):\n",
        "  !cp '/content/drive/MyDrive/AI_Challenge_advertisement_test.csv' test.csv\n",
        "\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "R8JHVad-L5tJ"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Load the Data***"
      ],
      "metadata": {
        "id": "6ZOztIk7R-cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = Img_Dataset(\n",
        "    test_df,\n",
        "    val_transform,\n",
        "    labels = False\n",
        ")"
      ],
      "metadata": {
        "id": "Yx7-m3rrPN0J"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = BATCH,\n",
        "    num_workers = 2,\n",
        "    shuffle = False\n",
        ")"
      ],
      "metadata": {
        "id": "SqmE0hSGRYbZ"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Predict***"
      ],
      "metadata": {
        "id": "wibKySHISDrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and store all the predictions\n",
        "\n",
        "all_probs, all_names = [], []\n",
        "model.eval()\n",
        "if len(test_loader) == 0:\n",
        "    print(\"Test loader is empty. No predictions will be made.\")\n",
        "else:\n",
        "    with torch.no_grad():\n",
        "        for imgs, names in test_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            outputs = model(imgs)\n",
        "            probs = torch.sigmoid(outputs).squeeze(1).cpu().numpy()\n",
        "            all_probs.extend(probs)\n",
        "            all_names.extend(names)"
      ],
      "metadata": {
        "id": "4aAtGMwySKFq"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gathering all of them, and writing them in a right format\n",
        "\n",
        "# Check if all_probs is empty before concatenating\n",
        "if not all_probs:\n",
        "    preds = np.array([])\n",
        "else:\n",
        "    preds = [int(prob>= best_t) for prob in all_probs]  # use best threshold\n",
        "\n",
        "\n",
        "solution = pd.DataFrame({\n",
        "    \"image\": all_names,\n",
        "    \"class\": preds\n",
        "})\n",
        "\n",
        "# # Make sure solution order matches the test.csv\n",
        "# solution = test_df[[\"image\"]].merge(solution, on=\"image\", how=\"left\")\n",
        "# solution[\"class\"] = solution[\"class\"].fillna(0).astype(int)\n",
        "\n",
        "solution.to_csv(\"solution.csv\", index=False)\n",
        "print(\"Saved solution.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibJiSQfzSOAd",
        "outputId": "aa4560f5-961d-40bf-fbfa-c0ddf24f913d"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved solution.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDK why, but now when testing the score differs a lot"
      ],
      "metadata": {
        "id": "rWxv6Wmkrwc7"
      }
    }
  ]
}